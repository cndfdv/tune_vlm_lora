# Fine-tuning LLaVA-Gemma на русском языке

Проект по дообучению мультимодальной модели LLaVA-Gemma-2B на русскоязычных данных с использованием LoRA (Low-Rank Adaptation). Реализован полный пайплайн от загрузки данных до оценки качества модели на специализированных бенчмарках.

## О проекте

Мультимодальные модели, способные понимать и текст, и изображения, активно развиваются, но большинство из них ориентированы на английский язык. Этот проект демонстрирует подход к адаптации таких моделей для работы с русским языком через эффективное дообучение с помощью LoRA.

## Основные возможности

- **Эффективное дообучение** с использованием LoRA (только 3% параметров модели)
- **Работа с русским языком** с учетом морфологических особенностей
- **Полный пайплайн** от загрузки данных до оценки результатов
- **Оптимизация памяти** через gradient checkpointing и FP16
- **Воспроизводимые результаты** с фиксацией seed

## Структура проекта

```
.
├── main.ipynb              # Основной ноутбук с полным пайплайном
├── requirements.txt        # Зависимости проекта
├── results_comparison2.png # Визуализация результатов
└── README.md              # Этот файл
```

### main.ipynb - основной ноутбук

Содержит:
- Загрузку и тестирование базовой модели
- Подготовку данных для дообучения
- Дообучение с использованием LoRA
- Оценку на бенчмарках GQA-ru и MMBench-ru
- Сравнительный анализ результатов

## Технологический стек

### Модель

- **Базовая модель**: `deepvk/llava-gemma-2b-lora`
- **Архитектура**: LLaVA на базе Gemma-2B
- **Vision Encoder**: CLIP ViT (заморожен при дообучении)
- **Language Model**: Gemma-2B с LoRA-адаптацией

### Датасеты

Все датасеты доступны на Hugging Face от VK:

- **`deepvk/LLaVA-Instruct-ru`** - для дообучения
  - 109,905 тренировочных примеров
  - Диалоги с привязкой к изображениям COCO

- **`deepvk/GQA-ru`** - бенчмарк для оценки
  - Визуальные вопросы с краткими ответами
  - Метрика: ExactMatch с лемматизацией

- **`deepvk/MMBench-ru`** - бенчмарк для оценки
  - Multiple choice задачи
  - Метрика: Accuracy

## Методология

### LoRA конфигурация

```python
LoRA Config:
- rank: 64
- alpha: 128
- dropout: 0.05
- target_modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- trainable_params: ~88M (3.0%)
```

### Гиперпараметры обучения

```python
Training Config:
- epochs: 1
- batch_size: 1 (per device)
- gradient_accumulation: 8
- learning_rate: 1e-4
- scheduler: cosine with warmup
- precision: FP16
- train_samples: 2000
- eval_samples: 25
```

## Результаты

### Сравнение с baseline

| Бенчмарк | Baseline | После дообучения | Изменение |
|----------|----------|------------------|-----------|
| **GQA-ru** (ExactMatch) | 46.37% | 46.37% | +0.8% |
| **MMBench-ru** (Accuracy) | 40.19% | 62.20% | **+52.3%** |

### Ключевые наблюдения

- **GQA-ru**: Стабильные результаты, модель сохранила качество на кратких визуальных вопросах
- **MMBench-ru**: Существенное улучшение (+22 п.п.) на задачах multiple-choice, что указывает на успешную адаптацию к инструкционному формату
- Дообучение не приводит к деградации качества (catastrophic forgetting)

## Требования

- Python 3.10+
- PyTorch с поддержкой CUDA (рекомендуется CUDA 11.8+)
- GPU с минимум 16 GB VRAM
- ~50 GB дискового пространства (модель + COCO изображения)

## Установка и запуск

### 1. Клонирование репозитория

```bash
git clone <your-repo-url>
cd vk_practice_rudn
```

### 2. Установка зависимостей

```bash
pip install -r requirements.txt
```

### 3. Подготовка данных

Скачайте изображения COCO 2017 и поместите их в папку `coco/`:

```
coco/
├── train2017/  # Тренировочные изображения
└── val2014/    # Валидационные изображения
```

Ссылки для скачивания:
- [COCO 2017 Train](http://images.cocodataset.org/zips/train2017.zip)
- [COCO 2014 Val](http://images.cocodataset.org/zips/val2014.zip)

### 4. Запуск

Откройте `main.ipynb` и выполняйте ячейки последовательно. Результаты обучения сохраняются в `llava-gemma-finetuned_w_torchvision/`.

## Технические детали

### Особенности реализации

- `torchvision.read_image` для эффективной загрузки изображений
- Кастомный `LLaVADataCollator` для обработки мультимодальных данных
- Автоматическая заморозка vision encoder
- Оптимизация памяти CUDA
- Лемматизация для русского языка (pymorphy2)

### Воспроизводимость

Все эксперименты используют фиксированный seed (42):

```python
transformers.set_seed(42)
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
```

## Дальнейшее развитие

Возможные улучшения:

- [ ] Увеличение объема обучающей выборки
- [ ] Обучение в несколько эпох
- [ ] Подбор оптимальных гиперпараметров LoRA
- [ ] Эксперименты с различными learning rate
- [ ] Расширение набора бенчмарков
- [ ] Дообучение на специализированных доменах

## Лицензия

Этот проект использует открытые модели и датасеты. Пожалуйста, ознакомьтесь с лицензиями используемых ресурсов:
- [deepvk/llava-gemma-2b-lora](https://huggingface.co/deepvk/llava-gemma-2b-lora)
- [deepvk/LLaVA-Instruct-ru](https://huggingface.co/datasets/deepvk/LLaVA-Instruct-ru)

## Благодарности

Проект использует открытые модели и датасеты от [VK](https://huggingface.co/deepvk), за что им большая благодарность за вклад в развитие русскоязычного AI.
